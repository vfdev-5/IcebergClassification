{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(\"..\")\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    \n",
    "os.environ['PRETRAINED_MODELS'] = '../../../pretrained-models.pytorch/'\n",
    "os.environ['IGNITE_PATH'] = '../../../pytorch-ignite-master/'\n",
    "ignite_path = os.environ['IGNITE_PATH']\n",
    "if ignite_path not in sys.path:\n",
    "    sys.path.append(ignite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "  \"seed\": 7777,\n",
    "\n",
    "  \"n_epochs\": 200,\n",
    "  \"validate_every_epoch\": 4,\n",
    "\n",
    "  \"fold_index\": [0, 1, 2, 3, 4, 5, 6],\n",
    "  \"n_splits\": 7,\n",
    "\n",
    "  \"batch_size\": 32,\n",
    "  \"num_workers\": 12,\n",
    "\n",
    "  \"lr_schedulers\": [\n",
    "      {\"ExponentialLR\": {\"gamma\": 0.88 } },\n",
    "      {\"ReduceLROnPlateau\": {\"factor\": 0.2, \"mode\": \"max\",  \"patience\": 5, \"verbose\": \"True\"}}\n",
    "  ],\n",
    "  \"early_stopping\": {\"patience\": 10, \"mode\": \"max\"},\n",
    "\n",
    "  \"train_aug\": {\n",
    "    \"Compose\": {\n",
    "      \"transforms\": [{\n",
    "        \"RandomChoice\": {\n",
    "          \"transforms\": [{\n",
    "            \"RandomAffine\": {\n",
    "              \"translate\": [0.075, 0.075],\n",
    "              \"rotation\": [-90, 90],\n",
    "              \"interpolation\": 0}}, {\n",
    "            \"RandomFlip\": {\n",
    "              \"mode\": \"h\",\n",
    "              \"proba\": 0.5}}, {\n",
    "            \"RandomFlip\": {\n",
    "              \"mode\": \"v\",\n",
    "              \"proba\": 0.5\n",
    "            }}]}\n",
    "      }, { \"_ToTensor\": {}}]}\n",
    "  },\n",
    "\n",
    "  \"test_aug\": {\n",
    "    \"Compose\": {\n",
    "      \"transforms\": [{ \"_ToTensor\": {}}]}\n",
    "  },\n",
    "\n",
    "  \"optimizer\": {\n",
    "    \"Adam\": {\n",
    "      \"params\": [{\n",
    "        \"params\": {\n",
    "          \"model.features.parameters\": {}},\n",
    "          \"lr\": 0.001}, {\n",
    "        \"params\": {\n",
    "          \"model.classifier.parameters\": {}},\n",
    "          \"lr\": 0.001}]}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.dataflow import get_trainval_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, val_batches = get_trainval_batches(CONFIG['train_aug'], \n",
    "                                                  CONFIG['test_aug'], \n",
    "                                                  n_splits=CONFIG['n_splits'], \n",
    "                                                  fold_index=CONFIG['fold_index'], \n",
    "                                                  batch_size=CONFIG['batch_size'], \n",
    "                                                  num_workers=CONFIG['num_workers'], \n",
    "                                                  seed=CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.cuda.LongTensor'>\n",
      "<class 'torch.cuda.FloatTensor'> torch.Size([32, 2, 75, 75])\n",
      "<class 'torch.cuda.DoubleTensor'> torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_batches:\n",
    "    break\n",
    "print(type(batch_x), type(batch_y))\n",
    "for b in batch_x:\n",
    "    print(type(b), b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       "[torch.cuda.LongTensor of size 10 (GPU 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch_common_utils.nn_utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from common.models import IcebergSqueezeNet\n",
    "model = IcebergSqueezeNet(input_n_channels=1).cuda()\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.autograd.variable.Variable, torch.Size([5, 2]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.randn((5, 2, 75, 75)).cuda())\n",
    "y_pred = model(x, None)\n",
    "type(y_pred), y_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch_common_utils.training_utils import _apply_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.autograd.variable.Variable, torch.Size([32, 2]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_x, batch_y in train_batches:\n",
    "    batch_x = _apply_variable(batch_x, requires_grad=True)\n",
    "    batch_y = _apply_variable(batch_y, requires_grad=True)    \n",
    "    y_pred = model(*batch_x)\n",
    "    break\n",
    "type(y_pred), y_pred.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "criterion = CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  Variable containing:\n",
      " 0.6933\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_batches:\n",
    "    batch_x = _apply_variable(batch_x, requires_grad=True)\n",
    "    batch_y = _apply_variable(batch_y, requires_grad=False)    \n",
    "    y_pred = model(*batch_x)\n",
    "    loss = criterion(y_pred.data, batch_y)\n",
    "    print(\"Loss : \", loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch_common_utils.deserialization import restore_object, CustomObjectEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adam': {'params': [{'lr': 0.001,\n",
       "    'params': {'model.features.parameters': {}}},\n",
       "   {'lr': 0.001, 'params': {'model.classifier.parameters': {}}}]}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['optimizer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = CustomObjectEval(globals=globals())\n",
    "optimizer = restore_object(CONFIG['optimizer'], custom_objects=custom_objects, verbose_debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ExponentialLR': {'gamma': 0.88}},\n",
       " {'ReduceLROnPlateau': {'factor': 0.2,\n",
       "   'mode': 'max',\n",
       "   'patience': 5,\n",
       "   'verbose': 'True'}}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['lr_schedulers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_insert = {'optimizer': '_opt' }\n",
    "custom_objects = {\"_opt\": optimizer}\n",
    "\n",
    "lr_schedulers_conf = CONFIG['lr_schedulers']\n",
    "\n",
    "lr_schedulers = []\n",
    "if not isinstance(lr_schedulers_conf, (tuple, list)):\n",
    "    lr_schedulers_conf = [lr_schedulers_conf]\n",
    "    \n",
    "for s in lr_schedulers_conf:\n",
    "    scheduler = restore_object(s, params_to_insert=params_to_insert, custom_objects=custom_objects, verbose_debug=False)\n",
    "    lr_schedulers.append(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.optim.lr_scheduler.ExponentialLR at 0x7f244a091f98>,\n",
       " <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x7f244a091860>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'max', 'patience': 10}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"early_stopping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.torch_common_utils.training_utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(**CONFIG['early_stopping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with my scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "from common.torch_common_utils.training_utils import train_one_epoch, validate\n",
    "from common.torch_common_utils.training_utils import write_conf_log, write_csv_log, save_checkpoint, optimizer_to_str\n",
    "from common.torch_common_utils.training_utils import accuracy\n",
    "\n",
    "\n",
    "def accuracy_logits(y_logits, y_true):\n",
    "    y_pred = softmax(y_logits).data\n",
    "    return accuracy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOptimizer: Adam\\n- Param group: \\n\\tbetas: (0.9, 0.999)\\n\\teps: 1e-08\\n\\tlr: 0.0001293369914320991\\n\\tweight_decay: 0\\n\\tinitial_lr: 0.001\\n- Param group: \\n\\tbetas: (0.9, 0.999)\\n\\teps: 1e-08\\n\\tlr: 0.0001293369914320991\\n\\tweight_decay: 0\\n\\tinitial_lr: 0.001\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_to_str(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.abspath(os.path.join(\"..\", \"output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200: 100%|##########| 43/43 [00:02<00:00, 17.30it/s, Loss 261.3895 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.86it/s, Loss 200.0336 | accuracy_logits 0.469]\n",
      "Epoch: 2/200: 100%|##########| 43/43 [00:02<00:00, 18.40it/s, Loss 261.7551 | accuracy_logits 0.469]\n",
      "Epoch: 3/200: 100%|##########| 43/43 [00:02<00:00, 17.72it/s, Loss 261.7626 | accuracy_logits 0.469]\n",
      "Epoch: 4/200: 100%|##########| 43/43 [00:02<00:00, 17.85it/s, Loss 260.2074 | accuracy_logits 0.469]\n",
      "Epoch: 5/200: 100%|##########| 43/43 [00:02<00:00, 18.04it/s, Loss 261.1974 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  8.07it/s, Loss 201.6384 | accuracy_logits 0.464]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/200: 100%|##########| 43/43 [00:02<00:00, 17.39it/s, Loss 263.1954 | accuracy_logits 0.469]\n",
      "Epoch: 7/200: 100%|##########| 43/43 [00:02<00:00, 18.65it/s, Loss 262.9202 | accuracy_logits 0.469]\n",
      "Epoch: 8/200: 100%|##########| 43/43 [00:02<00:00, 17.86it/s, Loss 262.0109 | accuracy_logits 0.469]\n",
      "Epoch: 9/200: 100%|##########| 43/43 [00:02<00:00, 17.63it/s, Loss 261.6975 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.60it/s, Loss 199.9220 | accuracy_logits 0.469]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/200: 100%|##########| 43/43 [00:02<00:00, 17.40it/s, Loss 261.8768 | accuracy_logits 0.469]\n",
      "Epoch: 11/200: 100%|##########| 43/43 [00:02<00:00, 17.84it/s, Loss 261.8421 | accuracy_logits 0.469]\n",
      "Epoch: 12/200: 100%|##########| 43/43 [00:02<00:00, 17.94it/s, Loss 262.1969 | accuracy_logits 0.469]\n",
      "Epoch: 13/200: 100%|##########| 43/43 [00:02<00:00, 17.42it/s, Loss 263.6681 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.96it/s, Loss 201.5993 | accuracy_logits 0.464]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14/200: 100%|##########| 43/43 [00:02<00:00, 18.16it/s, Loss 262.8730 | accuracy_logits 0.469]\n",
      "Epoch: 15/200: 100%|##########| 43/43 [00:02<00:00, 17.75it/s, Loss 259.7109 | accuracy_logits 0.469]\n",
      "Epoch: 16/200: 100%|##########| 43/43 [00:02<00:00, 17.72it/s, Loss 261.0902 | accuracy_logits 0.469]\n",
      "Epoch: 17/200: 100%|##########| 43/43 [00:02<00:00, 18.00it/s, Loss 263.2315 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.79it/s, Loss 198.2152 | accuracy_logits 0.473]\n",
      "Epoch: 18/200: 100%|##########| 43/43 [00:02<00:00, 17.93it/s, Loss 259.7507 | accuracy_logits 0.469]\n",
      "Epoch: 19/200: 100%|##########| 43/43 [00:02<00:00, 17.64it/s, Loss 262.1835 | accuracy_logits 0.469]\n",
      "Epoch: 20/200: 100%|##########| 43/43 [00:02<00:00, 17.62it/s, Loss 262.6653 | accuracy_logits 0.469]\n",
      "Epoch: 21/200: 100%|##########| 43/43 [00:02<00:00, 17.49it/s, Loss 262.5035 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.59it/s, Loss 197.9460 | accuracy_logits 0.473]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22/200: 100%|##########| 43/43 [00:02<00:00, 17.58it/s, Loss 261.7564 | accuracy_logits 0.469]\n",
      "Epoch: 23/200: 100%|##########| 43/43 [00:02<00:00, 18.41it/s, Loss 261.5176 | accuracy_logits 0.469]\n",
      "Epoch: 24/200: 100%|##########| 43/43 [00:02<00:00, 18.10it/s, Loss 263.7052 | accuracy_logits 0.469]\n",
      "Epoch: 25/200: 100%|##########| 43/43 [00:02<00:00, 17.96it/s, Loss 260.7590 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.48it/s, Loss 201.4665 | accuracy_logits 0.464]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26/200: 100%|##########| 43/43 [00:02<00:00, 18.12it/s, Loss 262.7085 | accuracy_logits 0.469]\n",
      "Epoch: 27/200: 100%|##########| 43/43 [00:02<00:00, 17.37it/s, Loss 262.0637 | accuracy_logits 0.469]\n",
      "Epoch: 28/200: 100%|##########| 43/43 [00:02<00:00, 17.48it/s, Loss 262.6901 | accuracy_logits 0.469]\n",
      "Epoch: 29/200: 100%|##########| 43/43 [00:02<00:00, 17.82it/s, Loss 261.5629 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  8.28it/s, Loss 198.3979 | accuracy_logits 0.473]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30/200: 100%|##########| 43/43 [00:02<00:00, 16.92it/s, Loss 261.5695 | accuracy_logits 0.469]\n",
      "Epoch: 31/200: 100%|##########| 43/43 [00:02<00:00, 18.10it/s, Loss 262.8829 | accuracy_logits 0.469]\n",
      "Epoch: 32/200: 100%|##########| 43/43 [00:02<00:00, 17.24it/s, Loss 261.3990 | accuracy_logits 0.469]\n",
      "Epoch: 33/200: 100%|##########| 43/43 [00:02<00:00, 17.12it/s, Loss 262.4675 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.99it/s, Loss 198.4137 | accuracy_logits 0.473]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 34/200: 100%|##########| 43/43 [00:02<00:00, 17.31it/s, Loss 260.5088 | accuracy_logits 0.469]\n",
      "Epoch: 35/200: 100%|##########| 43/43 [00:02<00:00, 17.83it/s, Loss 261.2154 | accuracy_logits 0.469]\n",
      "Epoch: 36/200: 100%|##########| 43/43 [00:02<00:00, 17.74it/s, Loss 262.5144 | accuracy_logits 0.469]\n",
      "Epoch: 37/200: 100%|##########| 43/43 [00:02<00:00, 17.22it/s, Loss 260.5491 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.80it/s, Loss 198.3208 | accuracy_logits 0.473]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 5 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 38/200: 100%|##########| 43/43 [00:02<00:00, 17.74it/s, Loss 261.5128 | accuracy_logits 0.469]\n",
      "Epoch: 39/200: 100%|##########| 43/43 [00:02<00:00, 17.91it/s, Loss 262.0383 | accuracy_logits 0.469]\n",
      "Epoch: 40/200: 100%|##########| 43/43 [00:02<00:00, 17.66it/s, Loss 261.1984 | accuracy_logits 0.469]\n",
      "Epoch: 41/200: 100%|##########| 43/43 [00:02<00:00, 17.84it/s, Loss 261.8665 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.94it/s, Loss 201.6375 | accuracy_logits 0.464]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 6 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 42/200: 100%|##########| 43/43 [00:02<00:00, 18.04it/s, Loss 260.6991 | accuracy_logits 0.469]\n",
      "Epoch: 43/200: 100%|##########| 43/43 [00:02<00:00, 17.60it/s, Loss 263.2701 | accuracy_logits 0.469]\n",
      "Epoch: 44/200: 100%|##########| 43/43 [00:02<00:00, 17.86it/s, Loss 262.5660 | accuracy_logits 0.469]\n",
      "Epoch: 45/200: 100%|##########| 43/43 [00:02<00:00, 17.41it/s, Loss 263.8134 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.36it/s, Loss 200.0637 | accuracy_logits 0.469]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 46/200: 100%|##########| 43/43 [00:02<00:00, 18.29it/s, Loss 263.4254 | accuracy_logits 0.469]\n",
      "Epoch: 47/200: 100%|##########| 43/43 [00:02<00:00, 16.96it/s, Loss 261.9360 | accuracy_logits 0.469]\n",
      "Epoch: 48/200: 100%|##########| 43/43 [00:02<00:00, 18.10it/s, Loss 261.8886 | accuracy_logits 0.469]\n",
      "Epoch: 49/200: 100%|##########| 43/43 [00:02<00:00, 17.24it/s, Loss 262.8081 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.88it/s, Loss 199.9559 | accuracy_logits 0.469]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 8 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50/200: 100%|##########| 43/43 [00:02<00:00, 17.87it/s, Loss 260.8866 | accuracy_logits 0.469]\n",
      "Epoch: 51/200: 100%|##########| 43/43 [00:02<00:00, 18.12it/s, Loss 264.1410 | accuracy_logits 0.469]\n",
      "Epoch: 52/200: 100%|##########| 43/43 [00:02<00:00, 17.65it/s, Loss 261.9138 | accuracy_logits 0.469]\n",
      "Epoch: 53/200: 100%|##########| 43/43 [00:02<00:00, 17.94it/s, Loss 261.6674 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  8.45it/s, Loss 200.0092 | accuracy_logits 0.469]\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 54/200: 100%|##########| 43/43 [00:02<00:00, 17.36it/s, Loss 261.3426 | accuracy_logits 0.469]\n",
      "Epoch: 55/200: 100%|##########| 43/43 [00:02<00:00, 18.04it/s, Loss 260.4918 | accuracy_logits 0.469]\n",
      "Epoch: 56/200: 100%|##########| 43/43 [00:02<00:00, 17.36it/s, Loss 261.6993 | accuracy_logits 0.469]\n",
      "Epoch: 57/200: 100%|##########| 43/43 [00:02<00:00, 17.97it/s, Loss 263.2174 | accuracy_logits 0.469]\n",
      "Validation: 100%|##########| 7/7 [00:00<00:00,  7.71it/s, Loss 198.4539 | accuracy_logits 0.473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping: 10 / 10\n",
      "EarlyStopping: STOP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "best_acc = 0\n",
    "now = datetime.now()\n",
    "\n",
    "logs_path = os.path.join(output_path, 'logs', 'squeezenet_%s' % now.strftime(\"%Y%m%d_%H%M\"))\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)    \n",
    "    \n",
    "    \n",
    "config_str = \"\"\n",
    "for k, v in CONFIG.items():\n",
    "    config_str += \"{}: {}\\n\".format(k, v)\n",
    "write_conf_log(logs_path, config_str)\n",
    "    \n",
    "for epoch in range(0, CONFIG['n_epochs']):\n",
    "    \n",
    "    for scheduler in lr_schedulers:\n",
    "        if isinstance(scheduler, _LRScheduler):\n",
    "            scheduler.step()\n",
    "\n",
    "    # train for one epoch\n",
    "    ret = train_one_epoch(model, train_batches, criterion, optimizer, epoch, \n",
    "                          CONFIG['n_epochs'], \n",
    "                          avg_metrics=[accuracy_logits, ])\n",
    "    if ret is None:\n",
    "        break\n",
    "    loss, acc = ret\n",
    "\n",
    "    # evaluate on validation set\n",
    "    if epoch % CONFIG['validate_every_epoch'] == 0:\n",
    "        ret = validate(model, val_batches, criterion, avg_metrics=[accuracy_logits, ])\n",
    "        if ret is None:\n",
    "            break\n",
    "        val_loss, val_acc = ret\n",
    "        \n",
    "        for scheduler in lr_schedulers:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_acc)\n",
    "                \n",
    "        if early_stopping(val_acc):\n",
    "            break\n",
    " \n",
    "        # remember best accuracy and save checkpoint\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = max(val_acc, best_acc)\n",
    "            save_checkpoint(logs_path, 'val_acc', {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'optimizer' : optimizer.state_dict()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with [pytorch/ignite](https://github.com/pytorch/ignite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.trainer import Trainer, TrainingEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_update_function(batch):\n",
    "    \n",
    "    (batch_x, batch_a), batch_y = batch    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_x = Variable(batch_x)\n",
    "    batch_a = Variable(batch_a)\n",
    "    batch_y = Variable(batch_y)\n",
    "    \n",
    "    # compute output\n",
    "    batch_y_pred = model(batch_x)\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "\n",
    "def validation_inference_function(batch):\n",
    "    \n",
    "    (batch_x, batch_a), batch_y = batch    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_x = Variable(batch_x, volatile=True)\n",
    "    batch_a = Variable(batch_a, volatile=True)\n",
    "    batch_y = Variable(batch_y, volatile=True)\n",
    "    \n",
    "    batch_y_pred = model(batch_x)\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "    return loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('ignite')\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(training_data=train_batches, \n",
    "                  validation_data=val_batches, \n",
    "                  training_update_function=training_update_function, \n",
    "                  validation_inference_function=validation_inference_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_metrics(trainer, *args, **kwargs):\n",
    "    last_loss = trainer.training_history[-1]\n",
    "    trainer._logger.info(\"Loss {}\".format(last_loss))    \n",
    "\n",
    "trainer.add_event_handler(TrainingEvents.TRAINING_EPOCH_COMPLETED, display_training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training starting with params max_epochs=10 validate_every_epoch=2\n",
      "INFO:ignite.trainer.Trainer:Training starting with params max_epochs=10 validate_every_epoch=2\n",
      "Epoch[0] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[0] Complete. Time taken: 00:00:01\n",
      "Loss 0.691881000995636\n",
      "INFO:ignite.trainer.Trainer:Loss 0.691881000995636\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[1] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[1] Complete. Time taken: 00:00:01\n",
      "Loss 0.6950010657310486\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6950010657310486\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[2] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[2] Complete. Time taken: 00:00:01\n",
      "Loss 0.6961565613746643\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6961565613746643\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[3] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[3] Complete. Time taken: 00:00:01\n",
      "Loss 0.6936890482902527\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6936890482902527\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[4] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[4] Complete. Time taken: 00:00:01\n",
      "Loss 0.6931101679801941\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6931101679801941\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[5] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[5] Complete. Time taken: 00:00:01\n",
      "Loss 0.6946659088134766\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6946659088134766\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[6] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[6] Complete. Time taken: 00:00:01\n",
      "Loss 0.6952115297317505\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6952115297317505\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[7] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[7] Complete. Time taken: 00:00:01\n",
      "Loss 0.6934480667114258\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6934480667114258\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[8] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[8] Complete. Time taken: 00:00:01\n",
      "Loss 0.6953391432762146\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6953391432762146\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Epoch[9] Complete. Time taken: 00:00:01\n",
      "INFO:ignite.trainer.Trainer:Epoch[9] Complete. Time taken: 00:00:01\n",
      "Loss 0.6953180432319641\n",
      "INFO:ignite.trainer.Trainer:Loss 0.6953180432319641\n",
      "Validation Complete. Time taken: 00:00:00\n",
      "INFO:ignite.trainer.Trainer:Validation Complete. Time taken: 00:00:00\n",
      "Training complete. Time taken 00:00:16\n",
      "INFO:ignite.trainer.Trainer:Training complete. Time taken 00:00:16\n"
     ]
    }
   ],
   "source": [
    "trainer.run(max_epochs=CONFIG['n_epochs'], validate_every_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/setuptools/dist.py:349: UserWarning: Normalizing '0.1.0-alpha' to '0.1.0a0'\n",
      "  normalized_version,\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing ignite.egg-info/PKG-INFO\n",
      "writing dependency_links to ignite.egg-info/dependency_links.txt\n",
      "writing top-level names to ignite.egg-info/top_level.txt\n",
      "writing requirements to ignite.egg-info/requires.txt\n",
      "reading manifest file 'ignite.egg-info/SOURCES.txt'\n",
      "writing manifest file 'ignite.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/ignite\n",
      "copying build/lib/ignite/__init__.py -> build/bdist.linux-x86_64/egg/ignite\n",
      "copying build/lib/ignite/trainer.py -> build/bdist.linux-x86_64/egg/ignite\n",
      "byte-compiling build/bdist.linux-x86_64/egg/ignite/__init__.py to __init__.cpython-35.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/ignite/trainer.py to trainer.cpython-35.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying ignite.egg-info/zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/ignite-0.1.0a0-py3.5.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing ignite-0.1.0a0-py3.5.egg\n",
      "Copying ignite-0.1.0a0-py3.5.egg to /usr/local/lib/python3.5/dist-packages\n",
      "Adding ignite 0.1.0a0 to easy-install.pth file\n",
      "\n",
      "Installed /usr/local/lib/python3.5/dist-packages/ignite-0.1.0a0-py3.5.egg\n",
      "Processing dependencies for ignite==0.1.0a0\n",
      "Searching for enum34\n",
      "Reading https://pypi.python.org/simple/enum34/\n",
      "Download error on https://pypi.python.org/simple/enum34/: [Errno 101] Network is unreachable -- Some packages may not be found!\n",
      "Couldn't find index page for 'enum34' (maybe misspelled?)\n",
      "Scanning index of all packages (this may take a while)\n",
      "Reading https://pypi.python.org/simple/\n",
      "Download error on https://pypi.python.org/simple/: [Errno 101] Network is unreachable -- Some packages may not be found!\n",
      "No local packages or working download links found for enum34\n",
      "error: Could not find suitable distribution for Requirement.parse('enum34')\n"
     ]
    }
   ],
   "source": [
    "!cd ../../../pytorch-ignite-master/ && python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_config.json', 'w') as handler:\n",
    "    json.dump(CONFIG, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(filepath):\n",
    "    with open(filepath, 'r') as handler:\n",
    "        config = json.load(handler)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_dataflow.ipynb  data_visualization.ipynb\r\n",
      "check_training.ipynb  test_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(\"../scripts/train_configs/config_squeezenet_20171129_2155.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(\"test_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config == CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validate_every_epoch: 5\\nseed: 7777\\nnum_workers: 12\\nn_splits: 10\\nlr_scheduler: {\\n        \"ExponentialLR\": {            \\n            \"gamma\": 0.88\\n        }\\n    }\\n    \\nn_epochs: 200\\ntrain_aug: {\\n                    \"Compose\":\\n                        {\"transforms\": [{\"RandomChoice\": {\"transforms\": [\\n                                            {\"RandomAffine\": {\"translate\": [0.075, 0.075],\\n                                                              \"rotation\": [-90, 90],                                                \\n                                                              \"interpolation\": 0}},\\n                                            {\"RandomFlip\": {\"mode\": \"h\", \"proba\": 0.5}},\\n                                            {\"RandomFlip\": {\"mode\": \"v\", \"proba\": 0.5}}]}},\\n                                        {\"_ToTensor\": {}}\\n                                        ]\\n                        }\\n                    }\\n                \\ntest_aug: {\\n                        \"Compose\":\\n                            {\"transforms\": [{\"_ToTensor\": {}}]\\n                            }\\n                        }\\n                    \\nbatch_size: 32\\nstart_epoch: 0\\nfold_index: 0\\nval_size: 0.15\\noptimizer: {\"Adam\": {\\n                            \"params\": [{ \\n                                \"params\": {\"model.features.parameters\": {}},\\n                                \"lr\": 0.001\\n                            }, {\"params\": {\"model.classifier.parameters\": {}},\\n                                \"lr\": 0.001\\n                            }]}\\n                    }\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_str = \"\"\n",
    "for k, v in config.items():\n",
    "    config_str += \"{}: {}\\n\".format(k, v)\n",
    "config_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
